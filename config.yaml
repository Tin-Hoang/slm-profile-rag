# RAG Chatbot Configuration

# Profile Information
profile:
  name: "Tin Hoang"  # Change to your name
  title: "AI Research Engineer | MLOps Engineer"
  greeting: "Hi! I'm a chatbot trained on {name}'s professional background. Ask me anything about their experience, skills, and projects!"

# LLM Configuration
llm:
  provider: "ollama"  # Options: ollama, groq (auto-detected if GROQ_API_KEY is set)
  model: "llama3.2:3b"  # For Ollama. Other options: phi3:mini, gemma2:2b
  groq_model: "llama-3.3-70b-versatile"  # For Groq. Options: llama-3.1-8b-instant, mixtral-8x7b-32768, gemma2-9b-it
  temperature: 0.7
  max_tokens: 800  # Increased for better-formatted, complete responses
  top_p: 0.9

  # System prompt template
  system_prompt: |
    You are a knowledgeable AI assistant that helps answer questions about {name}, a {title}.
    You have been trained on their resume, project reports, LinkedIn profile, and professional documents.

    Your role is to provide accurate information about {name}'s:
    - Professional experience and work history
    - Technical skills and expertise
    - Projects and accomplishments
    - Education and certifications
    - Professional background and interests

    Communication Guidelines:
    - Be professional, helpful, and balanced in tone
    - Speak in third person about {name} (e.g., "{name} has experience in...", "His focus is on...")
    - Present information factually without over-selling
    - Stay grounded in the available context
    - Be honest about the limits of your knowledge

    When Information is Limited:
    - Be direct but not negative: "Based on the available profile, {name} has..."
    - Focus on what IS documented rather than what ISN'T
    - Suggest connecting directly for details: "For more specific details, it would be best to connect with {name} directly"
    - Avoid repeatedly saying "I don't have information" - instead briefly acknowledge and move forward
    - Keep responses concise and to the point

    Important Reminders:
    - You are an AI assistant, not the person himself - maintain this distinction
    - Only share information from the provided context - no fabrication
    - If uncertain, acknowledge it briefly: "The available information suggests..." or "Based on the profile..."
    - Keep responses realistic and professional (2-3 paragraphs max)
    - Avoid over-enthusiastic or salesy language

    CRITICAL FORMATTING REQUIREMENTS:
    You MUST follow these formatting rules strictly:

    1. LISTS - Each item MUST be on its own line:
       CORRECT:
       1. First item here
       2. Second item here
       3. Third item here

       WRONG (DO NOT DO THIS):
       Some text: 1. First item 2. Second item 3. Third item

    2. PARAGRAPHS - Always add a blank line between paragraphs

    3. EMPHASIS - Use **bold text** for names, roles, and key terms

    4. STRUCTURE - Break information into clear sections:
       - Start with a brief overview paragraph
       - Use numbered or bulleted lists for multiple items
       - End with a summary or suggestion if needed

    5. LINE BREAKS - Never put multiple sentences or list items on the same line without proper formatting

# Embedding Model Configuration
embeddings:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  # Alternative: "sentence-transformers/all-mpnet-base-v2" (better quality, slower)
  device: "cpu"  # Options: cpu, cuda, mps

# Vector Database Configuration
vectorstore:
  type: "chroma"
  collection_name: "profile_documents"
  persist_directory: "./chroma_db"

# Retrieval Strategy Configuration
# Extensible system supporting multiple retrieval approaches
retrieval:
  # Primary strategy selection
  # Options: vector, bm25, bm25_vector
  # Future: page_index, graph_vector
  strategy: "bm25_vector"

  # Final number of documents to return after retrieval/fusion
  final_k: 4

  # Vector search settings (used by: vector, bm25_vector, graph_vector)
  vector:
    enabled: true
    search_type: "similarity"  # similarity, mmr
    k: 10  # Docs to retrieve before fusion (when using hybrid)
    search_kwargs:
      fetch_k: 20  # For MMR
      lambda_mult: 0.5  # For MMR diversity

  # BM25 lexical search settings (used by: bm25, bm25_vector)
  bm25:
    enabled: true
    k: 10  # Docs to retrieve before fusion
    persist_path: "./bm25_index"
    tokenizer: "simple"  # Options: simple, nltk (requires nltk package)

  # Fusion settings (used by: bm25_vector, graph_vector, any multi-source strategy)
  fusion:
    algorithm: "rrf"  # Options: rrf (reciprocal rank fusion), weighted
    rrf_k: 60  # RRF constant (higher = less aggressive re-ranking)
    weights:
      vector: 0.7  # Weight for vector search results
      bm25: 0.3  # Weight for BM25 results
      graph: 0.0  # Reserved for future graph_vector strategy

  # Page Index settings (future: page_index strategy)
  # page_index:
  #   enabled: false
  #   model: "vidore/colpali-v1.2"
  #   persist_path: "./page_index"

  # Graph settings (future: graph_vector strategy)
  # graph:
  #   enabled: false
  #   store_type: "networkx"  # networkx, neo4j
  #   persist_path: "./graph_store"

# Document Processing Configuration
document_processing:
  # Chunking strategy
  chunk_size: 1000  # Characters per chunk
  chunk_overlap: 200  # Overlap between chunks

  # Supported file types
  supported_extensions:
    - .pdf
    - .docx
    - .doc
    - .html
    - .htm
    - .txt
    - .md

  # PDF processing
  pdf:
    extract_images: false

  # HTML processing
  html:
    parse_tables: true
    strip_tags: true

# Main Document Configuration
main_document:
  enabled: true  # Master switch for the feature
  path: "data/documents/main_profile.md"  # Path to main document
  max_tokens: 10000  # Maximum tokens allowed (generous limit)
  position: "before"  # Always before VectorDB context (high priority)

  # Auto-detect format from file extension
  # Supported: .md, .txt, .pdf, .docx, .html

  # Summarization settings
  summarize_if_exceeds: true  # Use LLM to summarize if > max_tokens
  summarization_target_tokens: 8000  # Target size after summarization
  summarization_prompt: |
    You are summarizing a professional profile document.
    Extract and preserve ALL critical information including:
    - Full name, title, and contact information
    - Current role and key responsibilities
    - Core technical skills and expertise areas
    - Major projects and accomplishments with metrics
    - Education and certifications
    - Professional background summary

    Maintain factual accuracy. Keep all numbers, dates, and specific achievements.
    Output a concise but comprehensive summary that captures the person's professional identity.

  # Caching
  cache_enabled: true  # Cache loaded content (reload only on file change)
  cache_check_interval: 60  # Check file modification time every N seconds

  # Error handling
  fail_silently: true  # Continue without main doc if loading fails
  fallback_to_vectordb_only: true  # Use only VectorDB if main doc unavailable

# RAG Pipeline Configuration
rag:
  # Reranking (optional, set to false if not using)
  use_reranking: false

  # Context window
  max_context_length: 3000  # Characters of context to include

  # Response settings
  include_sources: true  # Show source documents in response
  source_max_length: 150  # Max length of source preview
  enhance_responses: true  # Apply post-processing to improve tone and remove negative language

# UI Configuration
ui:
  page_title: "ðŸ’¬ Profile Q&A Chatbot"
  page_icon: "ðŸ¤–"
  layout: "centered"  # Options: centered, wide

  # Styling
  theme:
    primary_color: "#FF4B4B"
    background_color: "#FFFFFF"
    secondary_background_color: "#F0F2F6"
    text_color: "#262730"

  # Chat settings
  max_chat_history: 50
  show_timestamps: true

  # Example questions
  example_questions:
    - "What is {name}'s background?"
    - "What are {name}'s key technical skills?"
    - "Tell me about {name}'s recent projects"
    - "What is {name}'s educational background?"
    - "What kind of roles is {name} looking for?"

# Logging Configuration
logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "app.log"
  max_bytes: 10485760  # 10MB
  backup_count: 3
