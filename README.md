# ğŸ¤– SLM Profile RAG Chatbot

A RAG (Retrieval Augmented Generation) chatbot that answers questions about your professional profile using your resume, project reports, and other documents.

## âœ¨ Features

- ğŸ“„ **Multi-format Support**: Process PDF, Word, HTML, and text documents
- ğŸ§  **RAG Pipeline**: Semantic search with vector database (ChromaDB)
- ğŸ¦™ **Ollama Integration**: Run small language models locally
- ğŸ¨ **Clean UI**: Streamlit-based interface
- âš™ï¸ **Highly Configurable**: YAML-based settings for easy customization
- ğŸš€ **HuggingFace Spaces Ready**: Deploy with one click
- âœ¨ **Smart Response Enhancement**: Automatically removes negative language and adds professional, recruiter-friendly tone

## ğŸ—ï¸ Architecture

<p align="center">
  <img src="docs/arch_system_design.png" alt="System Architecture Diagram" width="800" />
</p>
<p align="center"><em>Overall system design showing the RAG pipeline and component interactions</em></p>

Detailed architecture can be found in [ARCHITECTURE.md](ARCHITECTURE.md).

## ğŸ› ï¸ Tech Stack

- **Python 3.10+** with UV package manager
- **LangChain** for RAG pipeline
- **ChromaDB** for vector storage
- **Ollama** for local LLM serving
- **Streamlit** for web interface
- **sentence-transformers** for embeddings
- **Ruff** for linting/formatting

## ğŸ“¦ Installation

### Prerequisites

1. **Python 3.10+**
2. **UV Package Manager**: Install via:
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```
3. **Ollama**: Install from [ollama.ai](https://ollama.ai)

### Setup

1. **Clone the repository**:
   ```bash
   git clone <your-repo-url>
   cd slm-profile-rag
   ```

2. **Install dependencies with UV**:
   ```bash
   uv venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   uv pip install -r requirements.txt
   ```

3. **Set up environment variables**:
   ```bash
   cp .env.template .env
   # Edit .env with your settings
   ```

4. **Configure the chatbot**:
   - Edit `config.yaml` to set your name, title, and preferences
   - Adjust model settings, chunking parameters, and system prompt

5. **Add your documents**:
   ```bash
   # Place your PDFs, Word docs, HTML files in:
   mkdir -p data/documents
   # Copy your resume, project reports, LinkedIn profile, etc.
   ```

6. **Pull Ollama model**:
   ```bash
   ollama pull llama3.2:3b
   # Or other small models: phi3:mini, gemma2:2b
   ```

## ğŸš€ Usage

### Local Development

1. **Process documents and build vector database**:
   ```bash
   python -m src.build_vectorstore
   ```

2. **Run the Streamlit app**:
   ```bash
   streamlit run app.py
   ```

3. **Open browser**: Navigate to `http://localhost:8501`

### Linting & Formatting

```bash
# Check code
uv run ruff check .

# Format code
uv run ruff format .

# Check and fix
uv run ruff check --fix .
```

## ğŸŒ Deployment to Hugging Face Spaces

### Option 1: Direct Upload

1. Create a new Space on [Hugging Face](https://huggingface.co/spaces)
2. Select "Streamlit" as SDK
3. Upload all files from this repository
4. Add your documents to `data/documents/`
5. The Space will automatically build and deploy

### Option 2: Git Integration

1. Create a new Space and connect to Git
2. Push this repository:
   ```bash
   git remote add hf https://huggingface.co/spaces/YOUR_USERNAME/SPACE_NAME
   git push hf main
   ```

### Important Notes for HF Spaces

- **Ollama in HF Spaces**: You'll need a persistent Ollama server or use the Space's GPU
- **Vector DB**: Pre-build your ChromaDB locally and include it (or rebuild on startup)
- **Memory**: Small models (3B-7B params) work best on free tier
- **Secrets**: Add API keys in Space settings if using alternative LLM providers

## âš™ï¸ Configuration

### `config.yaml` - Main Settings

```yaml
profile:
  name: "Your Name"  # â† Change this!
  title: "Your Title"

llm:
  model: "llama3.2:3b"  # Choose your model
  temperature: 0.7

document_processing:
  chunk_size: 1000
  chunk_overlap: 200
```

### `.env` - Environment Variables

```bash
OLLAMA_BASE_URL=http://localhost:11434
CHROMA_PERSIST_DIR=./chroma_db
```

## ğŸ“š Project Structure

```
slm-profile-rag/
â”œâ”€â”€ app.py                          # Streamlit app entry point
â”œâ”€â”€ pyproject.toml                  # UV/pip dependencies & config
â”œâ”€â”€ config.yaml                     # RAG & LLM settings
â”œâ”€â”€ .env.example                    # Environment variables template
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â””â”€â”€ documents/                  # Your profile documents
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_processor.py       # Load & chunk documents
â”‚   â”œâ”€â”€ vectorstore.py              # ChromaDB operations
â”‚   â”œâ”€â”€ llm_handler.py              # Ollama/LLM interface
â”‚   â”œâ”€â”€ rag_pipeline.py             # RAG chain logic
â”‚   â”œâ”€â”€ response_enhancer.py        # Response post-processing (NEW!)
â”‚   â”œâ”€â”€ config_loader.py            # Load config.yaml & .env
â”‚   â””â”€â”€ build_vectorstore.py        # CLI to build vector DB
â”œâ”€â”€ chroma_db/                      # Vector database (auto-generated)
â””â”€â”€ tests/                          # Unit tests
```

## ğŸ¯ Recommended Models for HF Spaces

| Model | Size | Speed | Quality | HF Spaces Tier |
|-------|------|-------|---------|----------------|
| `llama3.2:3b` | 3B | Fast | Good | Free âœ… |
| `phi3:mini` | 3.8B | Fast | Good | Free âœ… |
| `gemma2:2b` | 2B | Very Fast | Decent | Free âœ… |
| `llama3.1:8b` | 8B | Medium | Great | Upgraded GPU |

## ğŸ”§ Troubleshooting

### Ollama Connection Issues
```bash
# Check Ollama is running
ollama list

# Start Ollama service
ollama serve
```

### ChromaDB Errors
```bash
# Rebuild vector database
rm -rf chroma_db/
python -m src.build_vectorstore
```

### HuggingFace Spaces Issues
- Check logs in the Space's "Logs" tab
- Ensure `requirements.txt` is generated: `uv pip compile pyproject.toml -o requirements.txt`
- Verify GPU/CPU settings match your model size

## ğŸ“„ License

MIT License - see LICENSE file

---

**Note**: Remember to update `config.yaml` with your personal information before deploying!

