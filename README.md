# ğŸ¤– SLM Profile RAG Chatbot

<p align="center">
  <img src="docs/banner_image.png" alt="SLM Profile RAG Banner" width="100%" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/LangChain-ğŸ¦œ-121212?style=for-the-badge" alt="LangChain"/>
  <img src="https://img.shields.io/badge/ChromaDB-Vector_DB-FF6F61?style=for-the-badge" alt="ChromaDB"/>
  <img src="https://img.shields.io/badge/Ollama-ğŸ¦™-000000?style=for-the-badge" alt="Ollama"/>
  <img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white" alt="Streamlit"/>
  <img src="https://img.shields.io/badge/HuggingFace-FFD21E?style=for-the-badge&logo=huggingface&logoColor=black" alt="HuggingFace"/>
  <img src="https://img.shields.io/badge/Ruff-D7FF64?style=for-the-badge&logo=ruff&logoColor=black" alt="Ruff"/>
  <img src="https://img.shields.io/badge/UV-DE5FE9?style=for-the-badge&logo=astral&logoColor=white" alt="UV"/>
</p>

A RAG (Retrieval Augmented Generation) chatbot that answers questions about your professional profile using your resume, project reports, and other documents.

## âœ¨ Features

- ğŸ“„ **Multi-format Support**: Process PDF, Word, HTML, and text documents
- ğŸ§  **RAG Pipeline**: Semantic search with vector database (ChromaDB)
- ğŸ“Œ **Main Document Support**: Guaranteed context - critical information always available, auto-format detection
- ğŸ¦™ **Ollama Integration**: Run small language models locally
- ğŸ¨ **Clean UI**: Streamlit-based interface
- âš™ï¸ **Highly Configurable**: YAML-based settings for easy customization
- ğŸš€ **HuggingFace Spaces Ready**: Deploy with one click
- âœ¨ **Smart Response Enhancement**: Automatically removes negative language and adds professional, recruiter-friendly tone

## ğŸ—ï¸ Architecture

<p align="center">
  <img src="docs/arch_system_design.png" alt="System Architecture Diagram" width="800" />
</p>
<p align="center"><em>Overall system design showing the RAG pipeline and component interactions</em></p>

Detailed architecture can be found in [ARCHITECTURE.md](ARCHITECTURE.md).

## ğŸ› ï¸ Tech Stack

- **Python 3.10+** with UV package manager
- **LangChain** for RAG pipeline
- **ChromaDB** for vector storage
- **Ollama** for local LLM serving
- **Streamlit** for web interface
- **sentence-transformers** for embeddings
- **tiktoken** for token counting
- **Ruff** for linting/formatting

## ğŸ“¦ Installation

### Prerequisites

1. **Python 3.10+**
2. **UV Package Manager**: Install via:
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```
3. **Ollama**: Install from [ollama.ai](https://ollama.ai)

### Setup

1. **Clone the repository**:
   ```bash
   git clone <your-repo-url>
   cd slm-profile-rag
   ```

2. **Install dependencies with UV**:
   ```bash
   uv venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   uv pip install -r requirements.txt
   ```

3. **Set up environment variables**:
   ```bash
   cp .env.template .env
   # Edit .env with your settings
   ```

4. **Configure the chatbot**:
   - Edit `config.yaml` to set your name, title, and preferences
   - Adjust model settings, chunking parameters, and system prompt

5. **Add your documents**:
   ```bash
   # Place your PDFs, Word docs, HTML files in:
   mkdir -p data/documents
   # Copy your resume, project reports, LinkedIn profile, etc.
   ```

6. **Create your Main Profile Document** (ğŸ“Œ **Recommended**):

   The Main Document feature ensures critical information is **always** included in responses, never missed by vector search.

   ```bash
   # Create main profile document (any format: .md, .txt, .pdf, .docx, .html)
   nano data/documents/main_profile.md
   ```

   **Include essential information:**
   - Full name, title, contact info
   - Current role and key responsibilities
   - Core skills and expertise
   - Major projects with metrics
   - Education and certifications

   **Example structure:**
   ```markdown
   # Your Name
   **Title**: Your Professional Title
   **Email**: your.email@example.com

   ## Professional Summary
   Brief summary of your experience...

   ## Core Skills
   - Skill 1, Skill 2, Skill 3

   ## Current Role
   ### Company - Role (Dates)
   - Achievement 1 with metrics
   - Achievement 2 with metrics

   ## Education
   - Degree, University, Year
   ```

   **Why use this?**
   - âœ… **Guaranteed Context**: Critical info never missed by vector similarity search
   - âœ… **Priority Positioning**: Appears BEFORE retrieved chunks (higher LLM attention)
   - âœ… **Auto-Format Detection**: Supports MD, TXT, PDF, DOCX, HTML automatically
   - âœ… **Smart Token Management**: LLM-based summarization if content exceeds 10k tokens

   The feature is enabled by default in `config.yaml`. See [docs/MAIN_DOCUMENT_GUIDE.md](docs/MAIN_DOCUMENT_GUIDE.md) for advanced configuration.

7. **Pull Ollama model**:
   ```bash
   ollama pull llama3.2:3b
   # Or other small models: phi3:mini, gemma2:2b
   ```

## ğŸš€ Usage

### Local Development

1. **Process documents and build vector database**:
   ```bash
   python -m src.build_vectorstore
   ```

2. **Run the Streamlit app**:
   ```bash
   streamlit run app.py
   ```

3. **Open browser**: Navigate to `http://localhost:8501`

**Screenshot Example:**

<p align="center">
  <img src="docs/screenshot-example.png" alt="SLM Profile RAG UI Example" width="800" />
</p>
<p align="center"><em>Streamlit chatbot interface in action</em></p>

## ğŸŒ Deployment to Hugging Face Spaces

### Option 1: Direct Upload

1. Create a new Space on [Hugging Face](https://huggingface.co/spaces)
2. Select "Streamlit" as SDK
3. Upload all files from this repository
4. Add your documents to `data/documents/`
5. The Space will automatically build and deploy

### Option 2: Git Integration

1. Create a new Space and connect to Git
2. Push this repository:
   ```bash
   git remote add hf https://huggingface.co/spaces/YOUR_USERNAME/SPACE_NAME
   git push hf main
   ```

### Important Notes for HF Spaces

- **Ollama in HF Spaces**: You'll need a persistent Ollama server or use the Space's GPU
- **Vector DB**: Pre-build your ChromaDB locally and include it (or rebuild on startup)
- **Memory**: Small models (3B-7B params) work best on free tier
- **Secrets**: Add API keys in Space settings if using alternative LLM providers

## âš™ï¸ Configuration

### `config.yaml` - Main Settings

```yaml
profile:
  name: "Your Name"  # â† Change this!
  title: "Your Title"

llm:
  model: "llama3.2:3b"  # Choose your model
  temperature: 0.7

document_processing:
  chunk_size: 1000
  chunk_overlap: 200
```

### `.env` - Environment Variables

```bash
OLLAMA_BASE_URL=http://localhost:11434
CHROMA_PERSIST_DIR=./chroma_db
```

## ğŸ“š Project Structure

```
slm-profile-rag/
â”œâ”€â”€ app.py                          # Streamlit app entry point
â”œâ”€â”€ pyproject.toml                  # UV/pip dependencies & config
â”œâ”€â”€ config.yaml                     # RAG & LLM settings
â”œâ”€â”€ .env.example                    # Environment variables template
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â””â”€â”€ documents/                  # Your profile documents
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_processor.py       # Load & chunk documents
â”‚   â”œâ”€â”€ vectorstore.py              # ChromaDB operations
â”‚   â”œâ”€â”€ llm_handler.py              # Ollama/LLM interface
â”‚   â”œâ”€â”€ rag_pipeline.py             # RAG chain logic
â”‚   â”œâ”€â”€ main_document_loader.py     # Main document management (NEW!)
â”‚   â”œâ”€â”€ response_enhancer.py        # Response post-processing
â”‚   â”œâ”€â”€ config_loader.py            # Load config.yaml & .env
â”‚   â””â”€â”€ build_vectorstore.py        # CLI to build vector DB
â”œâ”€â”€ chroma_db/                      # Vector database (auto-generated)
â””â”€â”€ tests/                          # Unit tests
```

## ğŸ¯ Recommended Models for HF Spaces

| Model | Size | Speed | Quality | HF Spaces Tier |
|-------|------|-------|---------|----------------|
| `llama3.2:3b` | 3B | Fast | Good | Free âœ… |
| `phi3:mini` | 3.8B | Fast | Good | Free âœ… |
| `gemma2:2b` | 2B | Very Fast | Decent | Free âœ… |
| `llama3.1:8b` | 8B | Medium | Great | Upgraded GPU |

## ğŸ” LLM Tracing with LangSmith

Monitor prompts, responses, and latency using [LangSmith](https://smith.langchain.com).

### Setup

1. **Get API Key**: Sign up at [smith.langchain.com](https://smith.langchain.com) (free tier: 5,000 traces/month)

2. **Configure `.env`**:
   ```bash
   LANGCHAIN_TRACING_V2=true
   LANGCHAIN_API_KEY=your_api_key_here
   LANGCHAIN_PROJECT=slm-profile-rag
   ```

3. **Restart the app**:
   ```bash
   streamlit run app.py
   ```

### What You Can See

- Full prompts sent to LLM (system prompt + context + question)
- Complete LLM responses
- Latency breakdown per step
- Token usage (when available)

## ğŸ”§ Troubleshooting

### Ollama Connection Issues
```bash
# Check Ollama is running
ollama list

# Start Ollama service
ollama serve
```

### ChromaDB Errors
```bash
# Rebuild vector database
rm -rf chroma_db/
python -m src.build_vectorstore
```

### HuggingFace Spaces Issues
- Check logs in the Space's "Logs" tab
- Ensure `requirements.txt` is generated: `uv pip compile pyproject.toml -o requirements.txt`
- Verify GPU/CPU settings match your model size

## ğŸ“„ License

MIT License - see LICENSE file

---

**Note**: Remember to update `config.yaml` with your personal information before deploying!
